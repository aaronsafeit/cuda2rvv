ğŸŒŸ Overview
cuda2rvv is a lightweight CUDA runtime shim and LLVM IR lowering toolkit designed to translate CUDA-style kernels to RISC-V Vector (RVV) instructions, enabling next-generation embedded vision, machine learning, and spatial AI systems to run on modern RISC-V hardware.

This project targets RISC-V platforms with RVV (RISC-V Vector Extension) and aims to provide a flexible, testable, and expandable infrastructure for building CUDA-compatible compute stacks on open hardware.

ğŸ¯ Project Goals
âœ… Translate CUDA kernels to RVV-compatible LLVM IR

âœ… Provide a runtime simulation of CUDA APIs on RISC-V

âœ… Support atomic ops, memory fences, warp intrinsics (__shfl_sync, __ballot_sync)

âœ… Enable texture memory fetch and surface writes

âœ… Support __shared__, __global__, __device__ qualifiers

âœ… Vectorized memory load/store, gather/scatter

âœ… Toolchain integration via custom LLVM IR passes


ğŸš€ Getting Started
1. Clone the repo
bash
Copy
Edit
git clone https://github.com/aaronsafeit/cuda2rvv.git
cd cuda2rvv
2. Build a Test Program
Use one of the provided test examples:

bash
Copy
Edit
g++ test_main.cpp -lpthread -o test_cuda2rvv
./test_cuda2rvv
3. Develop a CUDA-style kernel
Create a .cu file using __global__ functions and common CUDA APIs. Example provided in test_kernel.cu.

ğŸ§  Features
Feature	Status
__global__, __device__, __host__	âœ…
threadIdx.x, blockIdx.x	âœ…
__syncthreads()	âœ…
atomicAdd, atomicMin/Max	âœ…
Texture fetch/surface write	âœ… (host stub)
__shfl_sync, __ballot_sync	âœ…
Vector load/store	âœ…
Vector gather/scatter	âœ…
Warp reductions (sum, min, max)	âœ…
cudaMalloc, cudaFree	âœ…
cudaMemcpy, cudaMemcpyKind	âœ…
cudaMallocManaged (UM)	âœ…
RVV-aware memory fences	âœ…

ğŸ§ª Testing & Debugging
Drop your kernel in test_kernel.cu

Include it in test_main.cpp

Use unified memory and threading APIs to simulate parallelism

Output intermediate IR with Clang + LLVM for debugging:

bash
Copy
Edit
clang++ -S -emit-llvm -O1 -o kernel.ll test_kernel.cu
ğŸ›  LLVM Pass Integration
The project includes:

IR pass for CUDA intrinsic lowering to RVV intrinsics

Pattern matching using LLVMâ€™s PatternMatch API

Hook-in before llc or codegen phase for maximum compatibility

Coming soon:

Custom vector scheduling

Memory coalescing optimization

RVV-targeted warp-wide fusion


âš ï¸ Disclaimer
I have no idea what this code does.
Iâ€™ve never programmed a day in my life. 
If this works, it's a miracle. If it doesn't, I warned you.
Use at your own risk. Or don't. I wonâ€™t understand either way.

Maybe this will inspire one of us...

## ğŸ“œ License

MIT â€“ Free to use, modify, and distribute. Open hardware deserves open software.

